# Decoder-only-Transformer
A replica of the Transformer as proposed by the paper "Attention is all you need" https://arxiv.org/abs/1706.03762
This is a generative Transformer model that only has the decoder block in it, although with a slight modification of the masking in the self-attention head an encoder can be made. This is just an attempt at understanding how transformers work and is a very very small transformer with very low parameters. Please refer to the original paper to understand the code better. 

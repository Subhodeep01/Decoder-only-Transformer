{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMgiP5IUonkvU1qPi08URAP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#Creating a decoder-only **Transformer** from scratch\n","Based on \"Attention is all you need\" paper https://arxiv.org/abs/1706.03762"],"metadata":{"id":"mQeED4lT_ugo"}},{"cell_type":"markdown","source":["## *All the important imports*"],"metadata":{"id":"R6MFQI0TbhHV"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n","torch.manual_seed(1337)\n","vocab_size = 64"],"metadata":{"id":"EJZupbP-blCN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Hyperparameters*"],"metadata":{"id":"eI7fLMFAcpgd"}},{"cell_type":"code","source":["# Hyperparameters\n","batch_size = 64 # how mant independent sequences will we process in parallel?\n","block_size = 256  # what is the max length for the predictions?\n","max_iters = 5000\n","eval_interval = 500\n","learning_rate = 3e-4\n","eval_iters = 200\n","n_embd = 384\n","n_head = 6\n","n_layer = 6\n","dropout = 0.2"],"metadata":{"id":"dy4dHPg2pTW9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *The Self-attention Head* (Decoder-only)"],"metadata":{"id":"gkphfvckbNjt"}},{"cell_type":"code","source":["class Head(nn.Module):\n","  \"\"\" one head of self-attention \"\"\"\n","  def __init__(self, head_size):\n","    super().__init__()\n","    self.key = nn.Linear(n_embd, head_size, bias=False)\n","    self.query = nn.Linear(n_embd, head_size, bias=False)\n","    self.value = nn.Linear(n_embd, head_size, bias=False)\n","    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, x):\n","    B,T,C = x.shape\n","    k = self.key(x) # (B,T,C)\n","    q = self.query(x) # (B,T,C)\n","    # compute attention scores (\"affinities\")\n","    wei = q @ k.transpose(-2,-1) * C**-0.5 # (B,T,C) @ (B,C,T) => (B,T,T)\n","    wei = wei.masked_fill(self.tril[:T,:T] == 0, float(\"-inf\")) # (B,T,T)\n","    wei = F.softmax(wei, dim = -1)  # (B,T,T)\n","    wei = self.dropout(wei)\n","    # prepare the weighted aggregation of the values\n","    v = self.value(x) # (B,T,C)\n","    out = wei @ v # (B,T,T) @ (B,T,C) => (B,T,C)\n","    return out"],"metadata":{"id":"nUaUP6n7bU6V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","  \"\"\" multiple heads of self-att in parallel \"\"\"\n","\n","  def __init__(self, num_heads, head_size):\n","    super().__init__()\n","    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","    self.proj = nn.Linear(n_embd, n_embd)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, x):\n","    out = torch.cat([h(x) for h in self.heads], dim =-1)\n","    out = self.dropout(self.proj(out))\n","    return out"],"metadata":{"id":"eiwKy2ojtCg-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","  \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","  def __init__(self, n_embd):\n","    super().__init__()\n","    self.net = nn.Sequential(\n","        nn.Linear(n_embd, n_embd * 4),\n","        nn.ReLU(),\n","        nn.Linear(n_embd * 4, n_embd)\n","        nn.Dropout(dropout)\n","    )\n","\n","  def forward(self, x):\n","    return self.net(x)"],"metadata":{"id":"l4im57rmAbO8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Block(nn.Module):\n","  \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","  def __init__(self, n_embd, n_head):\n","    # n_embd: embedding dimension, n_head: the number of heads we'd like\n","    super().__init__()\n","    head_size = n_embd//n_head\n","    self.sa = MultiHeadAttention(n_head, head_size)\n","    self.ffwd = FeedForward(n_embd)\n","    self.ln1 = nn.LayerNorm(n_embd)\n","    self.ln2 = nn.LayerNorm(n_embd)\n","\n","  def forward(self, x):\n","    x = x + self.sa(self.ln1(x))\n","    x = x + self.ffwd(self.ln2(x))\n","    return x"],"metadata":{"id":"uGD8Jp6rECf1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##*Embedding Layer*"],"metadata":{"id":"crQypv24ADeA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"90afXwdV3dhR"},"outputs":[],"source":["class BigramLanguageModel(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    #each token directly reads off the logits for the next token from a lookup table\n","    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","    self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n","    self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","  def forward(self, idx, targets= None):\n","    B,T = idx.shape\n","\n","    #idx and targets are both (B,T) tensor of integers\n","    token_emb = self.token_embedding_table(idx) # (B,T,C)\n","    pos_emb = self.position_embedding_table(torch.arrange(T, device=device)) # (T,C)\n","    x = token_emb + pos_emb # (B,T,C)\n","    x = self.blocks(x) # (B,T,C)\n","    x = self.ln_f(x) # (B,T,C)\n","    logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","    if targets == None:\n","      loss = None\n","    else:\n","      B,T,C = logits.shape\n","      logits = logits.view(B*T, C)\n","      targets = targets.view(B*T)\n","      loss = F.cross_entropy(logits, targets)\n","\n","    return logits, loss\n","\n","  def generate(self, idx, max_new_tokens):\n","    #idx is (B,T) array of indices in the current context\n","    for _ in range(max_new_tokens):\n","      #crop idx to the last block_size tokens\n","      idx_cond = idx[:, -block_size:]\n","      #get the predictions\n","      logits, loss = self(idx_cond)\n","      #focus only on the last time step\n","      logits = logits[:, -1, :] # becomes [B, C]\n","      #apply softmax to get probabilities\n","      probs = F.softmax(logits, dim=-1) # [B, C]\n","      #sample from the distribution\n","      idx_next = torch.multinomial(probs, num_samples = 1) # [B, 1]\n","      #append sampled index to the running sequence\n","      idx = torch.cat((idx, idx_next), dim = 1) # [B, T+1]\n","    return idx\n","\n","model = BigramLanguageModel(vocab_size)\n","m = model.to(device)\n","out = m(xb, yb)\n","print(out.shape)\n"]},{"cell_type":"code","source":["@torch.no_grad()\n","def estimate_loss():\n","  out = {}\n","  model.eval()\n","  for split in ['train', 'val']:\n","    losses = torch.zeroes(eval_iters)\n","    for k in range(eval_iters):\n","      X,Y = get_batch(split)\n","      logits, loss = model(X,Y)\n","      losses[k] = loss.item()\n","    out[split] = losses.mean()\n","  model.train()\n","  return out"],"metadata":{"id":"H2YYd7w6sEpF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *The Trainer* (using **AdamW** optimizer)"],"metadata":{"id":"EQSVVKmZTwnC"}},{"cell_type":"code","source":["optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"],"metadata":{"id":"jVQVr97ADYc4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for steps in range(1000):\n","\n","  #every once in a while evaluate the loss on train and val data:\n","  if iter % eval_interval == 0:\n","    losses = estimate_loss()\n","    print(f\"step {iter}; train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","  #sample a batch of data\n","  xb, yb = get_batch('train')\n","\n","  #evaluate the loss\n","  logits, loss = m(xb, yb)\n","  optimizer.zero_grad(set_to_none= True)\n","  loss.backward()\n","  optimizer.stop()\n"],"metadata":{"id":"OjhTHfajUJhr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["context = torch.zeros((1, 1), dtype = torch.long, device = device)\n","print(m.generate(context, max_new_tokens=100)[0].tolist())"],"metadata":{"id":"t6eVhR6Trnjd"},"execution_count":null,"outputs":[]}]}